{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "distilbert_weights = model.state_dict()\n",
    "\n",
    "embedding_dimension = 768\n",
    "num_heads = 6\n",
    "head_size = embedding_dimension // num_heads \n",
    "\n",
    "def layer_norm(x, weight, bias, eps=1e-6):\n",
    "    \n",
    "    mean = x.mean(dim=-1, keepdim=True)\n",
    "    std_dev = x.std(dim=-1, keepdim=True)\n",
    "    x_normalized = (x - mean) / (std_dev + eps)\n",
    "    output = weight * x_normalized + bias\n",
    "    \n",
    "    return output\n",
    "\n",
    "def get_head_tensor(X_expanded, layer, Q_K_or_V):\n",
    "    \n",
    "    #Weight matrix W_Q, W_K, or W_V\n",
    "    weight_matrix = distilbert_weights['transformer.layer.' + str(layer) + '.attention.' + Q_K_or_V.lower() + '_lin.weight']\n",
    "    head_divided_weight_matrix = weight_matrix.view(num_heads, head_size, embedding_dimension)\n",
    "\n",
    "    #Bias matrix b_Q, b_K, or b_V\n",
    "    bias_matrix = distilbert_weights['transformer.layer.' + str(layer) + '.attention.' + Q_K_or_V.lower() + '_lin.bias']\n",
    "    head_divided_bias_matrix = bias_matrix.view(num_heads, head_size)\n",
    "\n",
    "    # Multiply X with W_Q, W_K, or W_V\n",
    "    head_matrices = torch.matmul(X_expanded, head_divided_weight_matrix.transpose(1, 2)) + head_divided_bias_matrix.unsqueeze(1)\n",
    "\n",
    "    # Reshape to get the head tensor\n",
    "    head_matrices = head_matrices.squeeze(1)\n",
    "    \n",
    "    return head_matrices\n",
    "\n",
    "def embed(sentence):\n",
    "    \n",
    "    distilbert_weights = model.state_dict()\n",
    "    \n",
    "    # Tokenize the sentence\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    inputs = inputs[\"input_ids\"][0]\n",
    "    tokens_length = len(inputs)\n",
    "    \n",
    "    # Full token embeddings\n",
    "    W = distilbert_weights['embeddings.word_embeddings.weight']\n",
    "\n",
    "    # Sentence token embeddings\n",
    "    X = W[inputs]\n",
    "    \n",
    "    # Positional embeddings\n",
    "    P_full = distilbert_weights['embeddings.position_embeddings.weight']\n",
    "    P = P_full[:tokens_length, :]\n",
    "\n",
    "    # Add position embeddings to token embeddings\n",
    "    X = X + P\n",
    "\n",
    "    # Normalize\n",
    "    X = layer_norm(X, distilbert_weights['embeddings.LayerNorm.weight'], distilbert_weights['embeddings.LayerNorm.bias'])\n",
    "    \n",
    "    return X\n",
    "\n",
    "tokens_len = X.shape[0] #TODO: redundant, initialize when creating a class\n",
    "\n",
    "def attention(X, layer):\n",
    "    \n",
    "    # For pytorch broadcasting to work, we need to expand the tensor to (1, 9, 768)\n",
    "    X_expanded = X.unsqueeze(0)  # Shape: (1, 9, 768)\n",
    "    \n",
    "    # Query, Key, and Value heads\n",
    "    Q = get_head_tensor(X_expanded, layer, 'Q')\n",
    "    K = get_head_tensor(X_expanded, layer, 'K')\n",
    "    V = get_head_tensor(X_expanded, layer, 'V')\n",
    "\n",
    "    # Attention Weights\n",
    "    A = torch.softmax(torch.matmul(Q, K.transpose(1, 2) / torch.sqrt(torch.tensor(head_size).float())),dim=-1)\n",
    "\n",
    "    # Update V\n",
    "    V = torch.matmul(A, V)\n",
    "\n",
    "    # Concatenating the heads\n",
    "    V = V.view(tokens_len,embedding_dimension)\n",
    "\n",
    "    #Linear layer\n",
    "    W_out_lin = distilbert_weights['transformer.layer.' + str(layer) + '.attention.out_lin.weight']\n",
    "    b_out_lin = distilbert_weights['transformer.layer.' + str(layer) + '.attention.out_lin.bias']\n",
    "    b_out_lin_matrix = b_out_lin.repeat(tokens_len, 1)\n",
    "\n",
    "    residual = torch.matmul(V, W_out_lin) + b_out_lin_matrix  #TODO: Need to transpose W_out_lin as per copilot suggestion?\n",
    "\n",
    "    # Residual Connections\n",
    "    X = X + residual\n",
    "\n",
    "    # Normalize\n",
    "    W_sa = distilbert_weights['transformer.layer.' + str(layer) + '.sa_layer_norm.weight']\n",
    "    b_sa = distilbert_weights['transformer.layer.' + str(layer) + '.sa_layer_norm.bias']\n",
    "    X = layer_norm(X, W_sa, b_sa)\n",
    "    \n",
    "    return X\n",
    "\n",
    "def feed_forward(X, layer):\n",
    "    \n",
    "    # ff Linear 1\n",
    "    W_ff1 = distilbert_weights['transformer.layer.' + str(layer) + '.ffn.lin1.weight']\n",
    "    b_ff1 = distilbert_weights['transformer.layer.' + str(layer) + '.ffn.lin1.bias']\n",
    "    b_ff1_matrix = b_ff1.repeat(9, 1)\n",
    "\n",
    "    FF_data = torch.matmul(X, W_ff1.transpose(0,1) ) + b_ff1_matrix\n",
    "\n",
    "    # FF ReLU\n",
    "    FF_data = torch.relu(FF_data)\n",
    "\n",
    "    # FF Linear 2\n",
    "    W_ff2 = distilbert_weights['transformer.layer.' + str(layer) + '.ffn.lin2.weight']\n",
    "    b_ff2 = distilbert_weights['transformer.layer.' + str(layer) + '.ffn.lin2.bias']\n",
    "    b_ff2_matrix = b_ff2.repeat(9, 1)\n",
    "\n",
    "    X = torch.matmul(FF_data, W_ff2.transpose(0,1) ) + b_ff2_matrix\n",
    "\n",
    "    # Normalize\n",
    "    W_ff = distilbert_weights['transformer.layer.' + str(layer) + '.output_layer_norm.weight']\n",
    "    b_ff = distilbert_weights['transformer.layer.' + str(layer) + '.output_layer_norm.bias']\n",
    "    X = layer_norm(X, W_ff, b_ff)\n",
    "    \n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9836, -0.1272, -0.6853,  ...,  1.0959,  0.5143,  0.6237],\n",
      "        [ 0.2990, -0.2386,  0.7141,  ..., -0.2033,  0.0946,  0.0958],\n",
      "        [ 1.0037, -0.1444,  0.3477,  ..., -0.0689, -0.6273,  0.4990],\n",
      "        ...,\n",
      "        [ 1.1924, -0.2387,  1.2874,  ...,  1.0004, -0.8292, -0.3271],\n",
      "        [-0.2179,  0.1382,  1.5083,  ...,  0.2565,  1.0773, -0.4967],\n",
      "        [ 0.4736,  0.0803,  0.5177,  ...,  1.0466, -0.1284, -0.4433]])\n"
     ]
    }
   ],
   "source": [
    "X = embed(\"The cat sat on the mat.\")\n",
    "for layer in range(6):\n",
    "    X = attention(X, layer)\n",
    "    X = feed_forward(X, layer)\n",
    "print(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
