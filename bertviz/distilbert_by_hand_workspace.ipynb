{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare with Actual Distilbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from distilbert_by_hand import DistilBertByHand\n",
    "from transformers import DistilBertTokenizer, DistilBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7931, -0.2998, -0.5067,  ..., -0.7640, -0.0722,  0.8590],\n",
      "        [-0.2309,  0.9762,  0.4279,  ..., -0.0098,  0.1503,  0.1877],\n",
      "        [-0.3172, -0.0491, -0.2106,  ...,  0.0203,  0.4648,  1.4245],\n",
      "        ...,\n",
      "        [ 0.7091, -0.0809, -0.6096,  ..., -0.3787, -0.7070, -0.9288],\n",
      "        [ 0.3666, -0.3846, -0.2389,  ...,  0.7856,  0.6769,  0.5238],\n",
      "        [-0.1419,  0.5372,  0.7171,  ...,  0.3744,  0.3691,  0.2721]])\n",
      "tensor([[-0.2713, -0.0781, -0.0216,  ..., -0.0853,  0.4197,  0.1664],\n",
      "        [-0.2253,  0.0514, -0.1776,  ..., -0.0192,  1.0410, -0.4474],\n",
      "        [-0.1074, -0.0692,  0.1663,  ..., -0.2652,  0.3477,  0.3385],\n",
      "        ...,\n",
      "        [ 0.2359, -0.0804, -0.0106,  ..., -0.1970,  0.3074,  0.1018],\n",
      "        [ 0.3169, -0.1647, -0.3697,  ...,  0.0730,  0.0621, -0.6574],\n",
      "        [ 0.3583,  0.1961,  0.0775,  ..., -0.1091,  0.0866, -0.5101]])\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The cat sat on the mat.\"\n",
    "\n",
    "# My implementation\n",
    "my_transformer = DistilBertByHand()\n",
    "\n",
    "# Actual implementation\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "their_transformer = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "# Compare the outputs\n",
    "my_output = my_transformer(sentence)\n",
    "with torch.no_grad():  \n",
    "    their_output = their_transformer(**inputs)\n",
    "\n",
    "print(my_output)\n",
    "print(their_output[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertModel(\n",
       "  (embeddings): Embeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "their_transformer.eval()  # evaluation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old work below this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "distilbert_weights = model.state_dict()\n",
    "\n",
    "embedding_dimension = 768\n",
    "num_heads = 6\n",
    "head_size = embedding_dimension // num_heads \n",
    "\n",
    "def layer_norm(x, weight, bias, eps=1e-6):\n",
    "    \n",
    "    mean = x.mean(dim=-1, keepdim=True)\n",
    "    std_dev = x.std(dim=-1, keepdim=True)\n",
    "    x_normalized = (x - mean) / (std_dev + eps)\n",
    "    output = weight * x_normalized + bias\n",
    "    \n",
    "    return output\n",
    "\n",
    "def get_head_tensor(X_expanded, layer, Q_K_or_V):\n",
    "    \n",
    "    #Weight matrix W_Q, W_K, or W_V\n",
    "    weight_matrix = distilbert_weights['transformer.layer.' + str(layer) + '.attention.' + Q_K_or_V.lower() + '_lin.weight']\n",
    "    head_divided_weight_matrix = weight_matrix.view(num_heads, head_size, embedding_dimension)\n",
    "\n",
    "    #Bias matrix b_Q, b_K, or b_V\n",
    "    bias_matrix = distilbert_weights['transformer.layer.' + str(layer) + '.attention.' + Q_K_or_V.lower() + '_lin.bias']\n",
    "    head_divided_bias_matrix = bias_matrix.view(num_heads, head_size)\n",
    "\n",
    "    # Multiply X with W_Q, W_K, or W_V\n",
    "    head_matrices = torch.matmul(X_expanded, head_divided_weight_matrix.transpose(1, 2)) + head_divided_bias_matrix.unsqueeze(1)\n",
    "\n",
    "    # Reshape to get the head tensor\n",
    "    head_matrices = head_matrices.squeeze(1)\n",
    "    \n",
    "    return head_matrices\n",
    "\n",
    "def embed(sentence):\n",
    "    \n",
    "    distilbert_weights = model.state_dict()\n",
    "    \n",
    "    # Tokenize the sentence\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    inputs = inputs[\"input_ids\"][0]\n",
    "    tokens_length = len(inputs)\n",
    "    \n",
    "    # Full token embeddings\n",
    "    W = distilbert_weights['embeddings.word_embeddings.weight']\n",
    "\n",
    "    # Sentence token embeddings\n",
    "    X = W[inputs]\n",
    "    \n",
    "    # Positional embeddings\n",
    "    P_full = distilbert_weights['embeddings.position_embeddings.weight']\n",
    "    P = P_full[:tokens_length, :]\n",
    "\n",
    "    # Add position embeddings to token embeddings\n",
    "    X = X + P\n",
    "\n",
    "    # Normalize\n",
    "    X = layer_norm(X, distilbert_weights['embeddings.LayerNorm.weight'], distilbert_weights['embeddings.LayerNorm.bias'])\n",
    "    \n",
    "    return X\n",
    "\n",
    "tokens_len = X.shape[0] #TODO: redundant, initialize when creating a class\n",
    "\n",
    "def attention(X, layer):\n",
    "    \n",
    "    # For pytorch broadcasting to work, we need to expand the tensor to (1, 9, 768)\n",
    "    X_expanded = X.unsqueeze(0)  # Shape: (1, 9, 768)\n",
    "    \n",
    "    # Query, Key, and Value heads\n",
    "    Q = get_head_tensor(X_expanded, layer, 'Q')\n",
    "    K = get_head_tensor(X_expanded, layer, 'K')\n",
    "    V = get_head_tensor(X_expanded, layer, 'V')\n",
    "\n",
    "    # Attention Weights\n",
    "    A = torch.softmax(torch.matmul(Q, K.transpose(1, 2) / torch.sqrt(torch.tensor(head_size).float())),dim=-1)\n",
    "\n",
    "    # Update V\n",
    "    V = torch.matmul(A, V)\n",
    "\n",
    "    # Concatenating the heads\n",
    "    V = V.view(tokens_len,embedding_dimension)\n",
    "\n",
    "    #Linear layer\n",
    "    W_out_lin = distilbert_weights['transformer.layer.' + str(layer) + '.attention.out_lin.weight']\n",
    "    b_out_lin = distilbert_weights['transformer.layer.' + str(layer) + '.attention.out_lin.bias']\n",
    "    b_out_lin_matrix = b_out_lin.repeat(tokens_len, 1)\n",
    "\n",
    "    residual = torch.matmul(V, W_out_lin) + b_out_lin_matrix  #TODO: Need to transpose W_out_lin as per copilot suggestion?\n",
    "\n",
    "    # Residual Connections\n",
    "    X = X + residual\n",
    "\n",
    "    # Normalize\n",
    "    W_sa = distilbert_weights['transformer.layer.' + str(layer) + '.sa_layer_norm.weight']\n",
    "    b_sa = distilbert_weights['transformer.layer.' + str(layer) + '.sa_layer_norm.bias']\n",
    "    X = layer_norm(X, W_sa, b_sa)\n",
    "    \n",
    "    return X\n",
    "\n",
    "def feed_forward(X, layer):\n",
    "    \n",
    "    # ff Linear 1\n",
    "    W_ff1 = distilbert_weights['transformer.layer.' + str(layer) + '.ffn.lin1.weight']\n",
    "    b_ff1 = distilbert_weights['transformer.layer.' + str(layer) + '.ffn.lin1.bias']\n",
    "    b_ff1_matrix = b_ff1.repeat(9, 1)\n",
    "\n",
    "    FF_data = torch.matmul(X, W_ff1.transpose(0,1) ) + b_ff1_matrix\n",
    "\n",
    "    # FF ReLU\n",
    "    FF_data = torch.relu(FF_data)\n",
    "\n",
    "    # FF Linear 2\n",
    "    W_ff2 = distilbert_weights['transformer.layer.' + str(layer) + '.ffn.lin2.weight']\n",
    "    b_ff2 = distilbert_weights['transformer.layer.' + str(layer) + '.ffn.lin2.bias']\n",
    "    b_ff2_matrix = b_ff2.repeat(9, 1)\n",
    "\n",
    "    X = torch.matmul(FF_data, W_ff2.transpose(0,1) ) + b_ff2_matrix\n",
    "\n",
    "    # Normalize\n",
    "    W_ff = distilbert_weights['transformer.layer.' + str(layer) + '.output_layer_norm.weight']\n",
    "    b_ff = distilbert_weights['transformer.layer.' + str(layer) + '.output_layer_norm.bias']\n",
    "    X = layer_norm(X, W_ff, b_ff)\n",
    "    \n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9836, -0.1272, -0.6853,  ...,  1.0959,  0.5143,  0.6237],\n",
      "        [ 0.2990, -0.2386,  0.7141,  ..., -0.2033,  0.0946,  0.0958],\n",
      "        [ 1.0037, -0.1444,  0.3477,  ..., -0.0689, -0.6273,  0.4990],\n",
      "        ...,\n",
      "        [ 1.1924, -0.2387,  1.2874,  ...,  1.0004, -0.8292, -0.3271],\n",
      "        [-0.2179,  0.1382,  1.5083,  ...,  0.2565,  1.0773, -0.4967],\n",
      "        [ 0.4736,  0.0803,  0.5177,  ...,  1.0466, -0.1284, -0.4433]])\n"
     ]
    }
   ],
   "source": [
    "X = embed(\"The cat sat on the mat.\")\n",
    "for layer in range(6):\n",
    "    X = attention(X, layer)\n",
    "    X = feed_forward(X, layer)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9836, -0.1272, -0.6853,  ...,  1.0959,  0.5143,  0.6237],\n",
      "        [ 0.2990, -0.2386,  0.7141,  ..., -0.2033,  0.0946,  0.0958],\n",
      "        [ 1.0037, -0.1444,  0.3477,  ..., -0.0689, -0.6273,  0.4990],\n",
      "        ...,\n",
      "        [ 1.1924, -0.2387,  1.2874,  ...,  1.0004, -0.8292, -0.3271],\n",
      "        [-0.2179,  0.1382,  1.5083,  ...,  0.2565,  1.0773, -0.4967],\n",
      "        [ 0.4736,  0.0803,  0.5177,  ...,  1.0466, -0.1284, -0.4433]])\n"
     ]
    }
   ],
   "source": [
    "transformer = DistilBERTProcessor()\n",
    "X = transformer(\"The cat sat on the mat.\")\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilBERTProcessor:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "        self.distilbert_weights = self.model.state_dict()\n",
    "        self.embedding_dimension = 768\n",
    "        self.num_heads = 6\n",
    "        self.head_size = self.embedding_dimension // self.num_heads\n",
    "        self.tokens_len = None  # This will be initialized during embedding\n",
    "\n",
    "    def layer_norm(self, x, weight, bias, eps=1e-6):\n",
    "        \n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std_dev = x.std(dim=-1, keepdim=True)\n",
    "        x_normalized = (x - mean) / (std_dev + eps)\n",
    "        output = weight * x_normalized + bias\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def get_head_tensor(self, X_expanded, layer, Q_K_or_V):\n",
    "        \n",
    "        #Weight matrix W_Q, W_K, or W_V\n",
    "        weight_matrix = self.distilbert_weights['transformer.layer.' + str(layer) + '.attention.' + Q_K_or_V.lower() + '_lin.weight']\n",
    "        head_divided_weight_matrix = weight_matrix.view(self.num_heads, self.head_size, self.embedding_dimension)\n",
    "        \n",
    "        #Bias matrix b_Q, b_K, or b_V\n",
    "        bias_matrix = self.distilbert_weights['transformer.layer.' + str(layer) + '.attention.' + Q_K_or_V.lower() + '_lin.bias']\n",
    "        head_divided_bias_matrix = bias_matrix.view(self.num_heads, self.head_size)\n",
    "        \n",
    "        # Multiply X with W_Q, W_K, or W_V\n",
    "        head_matrices = torch.matmul(X_expanded, head_divided_weight_matrix.transpose(1, 2)) + head_divided_bias_matrix.unsqueeze(1)\n",
    "        \n",
    "        # Reshape to get the head tensor\n",
    "        head_matrices = head_matrices.squeeze(1)\n",
    "        \n",
    "        return head_matrices\n",
    "\n",
    "    def embed(self, sentence):\n",
    "        \n",
    "        # Tokenize the sentence\n",
    "        inputs = self.tokenizer(sentence, return_tensors=\"pt\")\n",
    "        inputs = inputs[\"input_ids\"][0]\n",
    "        \n",
    "        # Initialize tokens_len\n",
    "        self.tokens_len = len(inputs)\n",
    "        \n",
    "        # Full token embeddings\n",
    "        W = self.distilbert_weights['embeddings.word_embeddings.weight']\n",
    "        \n",
    "        # Sentence token embeddings\n",
    "        X = W[inputs]\n",
    "        \n",
    "        # Positional embeddings\n",
    "        P_full = self.distilbert_weights['embeddings.position_embeddings.weight']\n",
    "        P = P_full[:self.tokens_len, :]\n",
    "        \n",
    "        # Add position embeddings to token embeddings\n",
    "        X = X + P\n",
    "        \n",
    "        # Normalize\n",
    "        X = self.layer_norm(X, self.distilbert_weights['embeddings.LayerNorm.weight'], self.distilbert_weights['embeddings.LayerNorm.bias'])\n",
    "        \n",
    "        return X\n",
    "\n",
    "    def attention(self, X, layer):\n",
    "        \n",
    "        # For pytorch broadcasting to work, we need to expand the tensor to (1, self.token_length, 768)\n",
    "        X_expanded = X.unsqueeze(0)\n",
    "        \n",
    "        # Query, Key, and Value heads\n",
    "        Q = self.get_head_tensor(X_expanded, layer, 'Q')\n",
    "        K = self.get_head_tensor(X_expanded, layer, 'K')\n",
    "        V = self.get_head_tensor(X_expanded, layer, 'V')\n",
    "        \n",
    "        # Attention Weights\n",
    "        A = torch.softmax(torch.matmul(Q, K.transpose(1, 2) / torch.sqrt(torch.tensor(self.head_size).float())), dim=-1)\n",
    "        \n",
    "        # Update V\n",
    "        V = torch.matmul(A, V)\n",
    "        \n",
    "        # Concatenating the heads\n",
    "        V = V.view(self.tokens_len, self.embedding_dimension)\n",
    "        \n",
    "        # Linear layer\n",
    "        W_out_lin = self.distilbert_weights['transformer.layer.' + str(layer) + '.attention.out_lin.weight']\n",
    "        b_out_lin = self.distilbert_weights['transformer.layer.' + str(layer) + '.attention.out_lin.bias']\n",
    "        b_out_lin_matrix = b_out_lin.repeat(self.tokens_len, 1)\n",
    "        \n",
    "        residual = torch.matmul(V, W_out_lin) + b_out_lin_matrix #TODO: Need to transpose W_out_lin as per copilot suggestion?\n",
    "\n",
    "        # Residual Connections\n",
    "        X = X + residual\n",
    "        \n",
    "        # Normalize\n",
    "        W_sa = self.distilbert_weights['transformer.layer.' + str(layer) + '.sa_layer_norm.weight']\n",
    "        b_sa = self.distilbert_weights['transformer.layer.' + str(layer) + '.sa_layer_norm.bias']\n",
    "        \n",
    "        X = self.layer_norm(X, W_sa, b_sa)\n",
    "        \n",
    "        return X\n",
    "\n",
    "    def feed_forward(self, X, layer):\n",
    "        \n",
    "        # FF Linear 1\n",
    "        W_ff_l1 = self.distilbert_weights['transformer.layer.' + str(layer) + '.ffn.lin1.weight']\n",
    "        b_ff_l1 = self.distilbert_weights['transformer.layer.' + str(layer) + '.ffn.lin1.bias']\n",
    "        b_ff_l1_matrix = b_ff_l1.repeat(self.tokens_len, 1)\n",
    "        \n",
    "        FF_data = torch.matmul(X, W_ff_l1.transpose(0, 1)) + b_ff_l1_matrix\n",
    "        \n",
    "        # FF ReLU\n",
    "        FF_data = torch.relu(FF_data)\n",
    "        \n",
    "        # FF Linear 2\n",
    "        W_ff_l2 = self.distilbert_weights['transformer.layer.' + str(layer) + '.ffn.lin2.weight']\n",
    "        b_ff_l2 = self.distilbert_weights['transformer.layer.' + str(layer) + '.ffn.lin2.bias']\n",
    "        b_ff_l2_matrix = b_ff_l2.repeat(self.tokens_len, 1)\n",
    "        \n",
    "        X = torch.matmul(FF_data, W_ff_l2.transpose(0, 1)) + b_ff_l2_matrix\n",
    "        \n",
    "        # Normalize\n",
    "        W_ff = self.distilbert_weights['transformer.layer.' + str(layer) + '.output_layer_norm.weight']\n",
    "        b_ff = self.distilbert_weights['transformer.layer.' + str(layer) + '.output_layer_norm.bias']\n",
    "        \n",
    "        X = self.layer_norm(X, W_ff, b_ff)\n",
    "        \n",
    "        return X\n",
    "\n",
    "    def run_layers(self, X):\n",
    "        \n",
    "        for layer in range(6):\n",
    "        \n",
    "            X = self.attention(X, layer)\n",
    "            X = self.feed_forward(X, layer)\n",
    "            \n",
    "        return X\n",
    "    \n",
    "    def __call__(self, sentence):\n",
    "        \n",
    "        X = self.embed(sentence)\n",
    "        \n",
    "        X = self.run_layers(X)\n",
    "        \n",
    "        return X"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
