{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    DistilBertTokenizer, DistilBertModel,\n",
    "    BertTokenizer, BertModel, \n",
    "    RobertaTokenizer, RobertaModel, \n",
    "    GPT2Tokenizer, GPT2Model, \n",
    "    XLNetTokenizer, XLNetModel\n",
    ")\n",
    "\n",
    "models_dict = {\n",
    "    DistilBertModel: 'distilbert-base-uncased', # Simplest, hence first for experimentation\n",
    "    BertModel: 'bert-base-uncased',\n",
    "    RobertaModel: 'roberta-base',\n",
    "    GPT2Model: 'gpt2-medium',\n",
    "    XLNetModel: 'xlnet-base-cased'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_state(model_number):\n",
    "    \n",
    "    current_model = list(models_dict.keys())[model_number]\n",
    "    model_version = models_dict[current_model]\n",
    "    \n",
    "    model = current_model.from_pretrained(model_version)\n",
    "\n",
    "    weights = model.state_dict()\n",
    "\n",
    "    # If you want to see the names of the weights and their shapes:\n",
    "    for name, tensor in weights.items():\n",
    "        print(name, tensor.shape)\n",
    "        \n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.word_embeddings.weight torch.Size([30522, 768])\n",
      "embeddings.position_embeddings.weight torch.Size([512, 768])\n",
      "embeddings.LayerNorm.weight torch.Size([768])\n",
      "embeddings.LayerNorm.bias torch.Size([768])\n",
      "transformer.layer.0.attention.q_lin.weight torch.Size([768, 768])\n",
      "transformer.layer.0.attention.q_lin.bias torch.Size([768])\n",
      "transformer.layer.0.attention.k_lin.weight torch.Size([768, 768])\n",
      "transformer.layer.0.attention.k_lin.bias torch.Size([768])\n",
      "transformer.layer.0.attention.v_lin.weight torch.Size([768, 768])\n",
      "transformer.layer.0.attention.v_lin.bias torch.Size([768])\n",
      "transformer.layer.0.attention.out_lin.weight torch.Size([768, 768])\n",
      "transformer.layer.0.attention.out_lin.bias torch.Size([768])\n",
      "transformer.layer.0.sa_layer_norm.weight torch.Size([768])\n",
      "transformer.layer.0.sa_layer_norm.bias torch.Size([768])\n",
      "transformer.layer.0.ffn.lin1.weight torch.Size([3072, 768])\n",
      "transformer.layer.0.ffn.lin1.bias torch.Size([3072])\n",
      "transformer.layer.0.ffn.lin2.weight torch.Size([768, 3072])\n",
      "transformer.layer.0.ffn.lin2.bias torch.Size([768])\n",
      "transformer.layer.0.output_layer_norm.weight torch.Size([768])\n",
      "transformer.layer.0.output_layer_norm.bias torch.Size([768])\n",
      "transformer.layer.1.attention.q_lin.weight torch.Size([768, 768])\n",
      "transformer.layer.1.attention.q_lin.bias torch.Size([768])\n",
      "transformer.layer.1.attention.k_lin.weight torch.Size([768, 768])\n",
      "transformer.layer.1.attention.k_lin.bias torch.Size([768])\n",
      "transformer.layer.1.attention.v_lin.weight torch.Size([768, 768])\n",
      "transformer.layer.1.attention.v_lin.bias torch.Size([768])\n",
      "transformer.layer.1.attention.out_lin.weight torch.Size([768, 768])\n",
      "transformer.layer.1.attention.out_lin.bias torch.Size([768])\n",
      "transformer.layer.1.sa_layer_norm.weight torch.Size([768])\n",
      "transformer.layer.1.sa_layer_norm.bias torch.Size([768])\n",
      "transformer.layer.1.ffn.lin1.weight torch.Size([3072, 768])\n",
      "transformer.layer.1.ffn.lin1.bias torch.Size([3072])\n",
      "transformer.layer.1.ffn.lin2.weight torch.Size([768, 3072])\n",
      "transformer.layer.1.ffn.lin2.bias torch.Size([768])\n",
      "transformer.layer.1.output_layer_norm.weight torch.Size([768])\n",
      "transformer.layer.1.output_layer_norm.bias torch.Size([768])\n",
      "transformer.layer.2.attention.q_lin.weight torch.Size([768, 768])\n",
      "transformer.layer.2.attention.q_lin.bias torch.Size([768])\n",
      "transformer.layer.2.attention.k_lin.weight torch.Size([768, 768])\n",
      "transformer.layer.2.attention.k_lin.bias torch.Size([768])\n",
      "transformer.layer.2.attention.v_lin.weight torch.Size([768, 768])\n",
      "transformer.layer.2.attention.v_lin.bias torch.Size([768])\n",
      "transformer.layer.2.attention.out_lin.weight torch.Size([768, 768])\n",
      "transformer.layer.2.attention.out_lin.bias torch.Size([768])\n",
      "transformer.layer.2.sa_layer_norm.weight torch.Size([768])\n",
      "transformer.layer.2.sa_layer_norm.bias torch.Size([768])\n",
      "transformer.layer.2.ffn.lin1.weight torch.Size([3072, 768])\n",
      "transformer.layer.2.ffn.lin1.bias torch.Size([3072])\n",
      "transformer.layer.2.ffn.lin2.weight torch.Size([768, 3072])\n",
      "transformer.layer.2.ffn.lin2.bias torch.Size([768])\n",
      "transformer.layer.2.output_layer_norm.weight torch.Size([768])\n",
      "transformer.layer.2.output_layer_norm.bias torch.Size([768])\n",
      "transformer.layer.3.attention.q_lin.weight torch.Size([768, 768])\n",
      "transformer.layer.3.attention.q_lin.bias torch.Size([768])\n",
      "transformer.layer.3.attention.k_lin.weight torch.Size([768, 768])\n",
      "transformer.layer.3.attention.k_lin.bias torch.Size([768])\n",
      "transformer.layer.3.attention.v_lin.weight torch.Size([768, 768])\n",
      "transformer.layer.3.attention.v_lin.bias torch.Size([768])\n",
      "transformer.layer.3.attention.out_lin.weight torch.Size([768, 768])\n",
      "transformer.layer.3.attention.out_lin.bias torch.Size([768])\n",
      "transformer.layer.3.sa_layer_norm.weight torch.Size([768])\n",
      "transformer.layer.3.sa_layer_norm.bias torch.Size([768])\n",
      "transformer.layer.3.ffn.lin1.weight torch.Size([3072, 768])\n",
      "transformer.layer.3.ffn.lin1.bias torch.Size([3072])\n",
      "transformer.layer.3.ffn.lin2.weight torch.Size([768, 3072])\n",
      "transformer.layer.3.ffn.lin2.bias torch.Size([768])\n",
      "transformer.layer.3.output_layer_norm.weight torch.Size([768])\n",
      "transformer.layer.3.output_layer_norm.bias torch.Size([768])\n",
      "transformer.layer.4.attention.q_lin.weight torch.Size([768, 768])\n",
      "transformer.layer.4.attention.q_lin.bias torch.Size([768])\n",
      "transformer.layer.4.attention.k_lin.weight torch.Size([768, 768])\n",
      "transformer.layer.4.attention.k_lin.bias torch.Size([768])\n",
      "transformer.layer.4.attention.v_lin.weight torch.Size([768, 768])\n",
      "transformer.layer.4.attention.v_lin.bias torch.Size([768])\n",
      "transformer.layer.4.attention.out_lin.weight torch.Size([768, 768])\n",
      "transformer.layer.4.attention.out_lin.bias torch.Size([768])\n",
      "transformer.layer.4.sa_layer_norm.weight torch.Size([768])\n",
      "transformer.layer.4.sa_layer_norm.bias torch.Size([768])\n",
      "transformer.layer.4.ffn.lin1.weight torch.Size([3072, 768])\n",
      "transformer.layer.4.ffn.lin1.bias torch.Size([3072])\n",
      "transformer.layer.4.ffn.lin2.weight torch.Size([768, 3072])\n",
      "transformer.layer.4.ffn.lin2.bias torch.Size([768])\n",
      "transformer.layer.4.output_layer_norm.weight torch.Size([768])\n",
      "transformer.layer.4.output_layer_norm.bias torch.Size([768])\n",
      "transformer.layer.5.attention.q_lin.weight torch.Size([768, 768])\n",
      "transformer.layer.5.attention.q_lin.bias torch.Size([768])\n",
      "transformer.layer.5.attention.k_lin.weight torch.Size([768, 768])\n",
      "transformer.layer.5.attention.k_lin.bias torch.Size([768])\n",
      "transformer.layer.5.attention.v_lin.weight torch.Size([768, 768])\n",
      "transformer.layer.5.attention.v_lin.bias torch.Size([768])\n",
      "transformer.layer.5.attention.out_lin.weight torch.Size([768, 768])\n",
      "transformer.layer.5.attention.out_lin.bias torch.Size([768])\n",
      "transformer.layer.5.sa_layer_norm.weight torch.Size([768])\n",
      "transformer.layer.5.sa_layer_norm.bias torch.Size([768])\n",
      "transformer.layer.5.ffn.lin1.weight torch.Size([3072, 768])\n",
      "transformer.layer.5.ffn.lin1.bias torch.Size([3072])\n",
      "transformer.layer.5.ffn.lin2.weight torch.Size([768, 3072])\n",
      "transformer.layer.5.ffn.lin2.bias torch.Size([768])\n",
      "transformer.layer.5.output_layer_norm.weight torch.Size([768])\n",
      "transformer.layer.5.output_layer_norm.bias torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "weights = get_model_state(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.6649e-02, -6.6612e-02, -1.6329e-02, -4.2112e-02, -8.0348e-03,\n",
      "        -1.3965e-02, -6.3488e-02, -2.0491e-02, -8.5822e-03, -6.3427e-02,\n",
      "        -2.8296e-02, -3.3587e-02, -3.5466e-02, -5.2275e-03, -2.0351e-02,\n",
      "        -6.0686e-02, -5.0486e-02, -5.8112e-02, -2.1134e-02, -5.8061e-02,\n",
      "        -3.6556e-02, -3.8286e-02, -9.5839e-03, -2.8228e-02, -1.0817e-01,\n",
      "        -4.2421e-02, -6.7244e-03, -7.6137e-02,  1.3189e-02, -1.9380e-02,\n",
      "        -3.8669e-02, -1.0872e-02, -1.7320e-02, -3.3488e-02, -6.0760e-02,\n",
      "        -5.3773e-02, -3.7320e-02, -2.9963e-02, -5.9872e-02, -2.6235e-02,\n",
      "        -5.3190e-02, -3.6603e-02, -7.2672e-02, -3.5065e-02, -1.1630e-02,\n",
      "        -7.6393e-03, -1.0994e-02, -3.4178e-02, -3.4682e-02, -3.5877e-02,\n",
      "        -5.6536e-02, -4.5791e-02, -5.2554e-02,  1.3923e-01, -3.5378e-02,\n",
      "        -3.6677e-02, -2.9200e-02, -9.8809e-03, -2.6176e-02,  1.1668e-02,\n",
      "        -2.1027e-02, -2.2904e-02,  1.5897e-01, -3.1597e-02, -3.2808e-02,\n",
      "         1.5736e-04, -4.1507e-02, -3.9246e-02, -1.6017e-02, -1.8613e-02,\n",
      "        -5.9349e-02, -1.7540e-02, -2.8575e-02, -6.7742e-02, -6.8971e-02,\n",
      "        -2.6818e-02, -6.8994e-02,  1.0464e-01, -3.0625e-02, -6.4972e-02,\n",
      "        -3.2487e-02, -3.4623e-02, -1.2663e-02, -2.3078e-02, -2.3987e-02,\n",
      "        -3.1393e-02, -1.9230e-02, -6.6155e-02, -2.7756e-02,  9.3243e-04,\n",
      "        -6.8342e-02, -9.6600e-03, -2.5876e-02, -2.7445e-02, -2.6773e-02,\n",
      "        -5.2506e-02,  1.3947e-01, -3.7640e-02, -1.2929e-02, -4.1518e-02,\n",
      "        -1.5133e-02, -4.2788e-02, -6.4663e-03, -5.8383e-02, -4.8563e-02,\n",
      "         1.2420e-01, -4.7650e-02, -3.1373e-02,  2.9081e-03, -2.0872e-02,\n",
      "        -4.5870e-02, -3.3865e-02, -8.0383e-03, -2.9120e-02, -3.9625e-02,\n",
      "        -1.2675e-02, -2.8485e-02, -3.6255e-02, -5.8316e-02, -3.6812e-02,\n",
      "        -2.6875e-02,  2.0824e-01, -1.9453e-02, -1.8921e-02, -1.4617e-01,\n",
      "        -2.7327e-02, -2.0323e-02, -1.7221e-01, -2.2653e-02, -4.1435e-02,\n",
      "         5.4807e-03, -3.6035e-02,  1.6259e-01,  1.1791e-02, -5.9787e-02,\n",
      "        -2.3837e-02, -2.9218e-02, -3.5700e-02, -1.4282e-02,  1.1917e-01,\n",
      "        -4.0071e-02, -5.2176e-02, -5.2419e-02, -1.8832e-02, -4.3448e-02,\n",
      "        -2.5695e-02, -1.8250e-02, -7.5523e-02, -7.3690e-02, -4.8426e-02,\n",
      "        -3.3186e-02, -3.4711e-02, -1.2745e-01, -3.7282e-02, -6.6504e-02,\n",
      "        -4.9192e-02, -1.4455e-02, -2.6551e-02, -3.3528e-02,  1.6467e-01,\n",
      "        -4.1863e-02, -2.7208e-02, -3.6358e-02, -6.1143e-02, -3.1592e-02,\n",
      "        -3.1196e-02, -3.2631e-02, -1.2978e-02,  1.8940e-01, -2.4032e-02,\n",
      "        -5.6532e-02, -5.6291e-03, -1.7708e-02,  5.5577e-03, -5.7207e-03,\n",
      "         2.1560e-01, -3.4072e-02, -3.2182e-02, -1.7952e-03, -1.2580e-02,\n",
      "        -1.9534e-02, -3.6569e-02, -7.1265e-02, -1.9859e-02, -4.0285e-02,\n",
      "        -5.8362e-02, -1.2554e-02, -4.5774e-02, -2.8333e-02, -2.5720e-02,\n",
      "        -2.6628e-02, -3.4098e-02, -9.3244e-02, -2.6549e-02, -4.8240e-02,\n",
      "        -5.1688e-02, -4.6563e-02, -5.2804e-02, -5.5591e-02, -2.9433e-02,\n",
      "        -5.4102e-02, -2.9153e-02,  1.2936e-01, -5.4297e-02, -2.2614e-02,\n",
      "        -2.3051e-02, -4.5075e-02, -4.3044e-02, -2.2817e-02, -4.7138e-02,\n",
      "        -3.3262e-02, -1.1252e-02, -3.5519e-02, -4.7457e-02, -7.1487e-03,\n",
      "         1.5421e-01,  5.8134e-02, -8.5862e-03,  1.2706e-01, -3.5591e-02,\n",
      "        -3.4042e-02, -1.6774e-02, -3.2135e-02, -3.7683e-02, -6.5761e-02,\n",
      "         8.8911e-02, -4.7724e-02, -4.6920e-02, -5.3790e-02, -4.6146e-02,\n",
      "        -4.8409e-02, -5.3394e-02, -9.0214e-02, -4.3760e-02, -9.8683e-03,\n",
      "        -2.2328e-02, -1.2033e-02,  9.2071e-03, -2.1769e-02, -3.2014e-02,\n",
      "        -3.8788e-02, -2.6640e-02, -5.8899e-02, -5.4094e-02, -3.6444e-02,\n",
      "         1.1415e-01, -1.2146e-02, -7.0879e-02, -2.0564e-02, -5.7028e-03,\n",
      "        -4.8117e-02, -2.8773e-02, -3.9924e-02,  1.4693e-02, -3.1839e-02,\n",
      "         1.1882e-02, -3.7452e-02, -4.9095e-02, -8.8911e-03, -3.4756e-02,\n",
      "        -2.8072e-02, -6.3319e-02, -3.0059e-02, -2.3367e-02, -7.3439e-02,\n",
      "        -3.3050e-02, -2.6863e-02, -5.1258e-02, -2.9975e-02, -2.9202e-02,\n",
      "        -6.7395e-02, -3.2151e-02, -2.4956e-03, -4.8538e-02, -3.6353e-02,\n",
      "        -2.1409e-02, -3.4312e-02, -5.7207e-02, -2.5087e-02, -6.0694e-02,\n",
      "        -4.4022e-02, -4.4966e-02, -3.3947e-02,  1.2171e-01, -7.9323e-03,\n",
      "        -4.7591e-02, -4.2654e-02, -4.5513e-02, -4.4978e-02, -4.3363e-02,\n",
      "        -6.6250e-02,  1.2313e-01, -8.8966e-06, -2.9052e-02, -5.7309e-02,\n",
      "        -3.0400e-02,  1.1279e-01, -3.8686e-02, -8.8789e-02, -2.7120e-02,\n",
      "        -4.0726e-02, -3.9209e-02, -3.0926e-02, -2.9425e-02, -6.6602e-02,\n",
      "        -2.9177e-02, -1.8975e-02, -4.0236e-02, -2.9138e-02, -7.6128e-02,\n",
      "        -3.1776e-02,  1.5368e-01, -5.1486e-02, -4.2500e-02, -5.2970e-02,\n",
      "        -6.7010e-02, -6.2748e-02, -2.3715e-02, -3.2212e-02,  1.4152e-01,\n",
      "        -8.2471e-02, -2.3625e-02, -3.0309e-02, -5.3621e-02, -6.9649e-02,\n",
      "        -1.7803e-02, -2.3769e-02, -6.4111e-02, -7.3562e-03, -6.0961e-03,\n",
      "        -3.7924e-02,  1.2052e-01, -6.9256e-02, -3.0765e-02, -1.6599e-02,\n",
      "        -4.4468e-02, -5.8563e-03, -5.7350e-02, -2.8163e-02, -2.7919e-02,\n",
      "        -6.6956e-02, -5.1107e-02, -3.4651e-02, -2.4526e-02, -3.4556e-03,\n",
      "         1.5953e-01, -4.1627e-02, -7.9780e-02, -3.2657e-02, -2.1868e-02,\n",
      "        -8.9877e-02, -4.1234e-02, -2.8382e-02,  1.4471e-01, -6.3259e-02,\n",
      "        -3.9905e-02, -2.5414e-02, -4.6677e-02, -2.1171e-02, -3.2632e-02,\n",
      "        -1.5562e-03, -3.6642e-02, -2.8094e-02, -3.1868e-02, -2.0083e-02,\n",
      "        -4.7520e-02, -5.0075e-02, -4.1033e-02, -3.4372e-02, -5.5971e-02,\n",
      "        -2.3304e-02, -6.5328e-02, -4.9872e-03, -2.0597e-02, -7.7594e-02,\n",
      "        -7.4935e-02, -3.6320e-02, -4.1358e-02, -2.8938e-02, -1.3375e-02,\n",
      "        -1.5624e-02, -1.5526e-02, -7.5439e-02, -2.4564e-02, -3.0878e-02,\n",
      "        -2.6489e-02, -3.2814e-02, -3.9868e-02, -2.2801e-02, -1.2313e-02,\n",
      "        -5.6333e-02, -3.2228e-02, -5.2696e-02, -4.1699e-02, -3.5892e-02,\n",
      "        -2.8940e-02, -3.4460e-02, -4.1481e-02, -7.0661e-02, -3.9967e-02,\n",
      "        -1.5033e-02, -1.3864e-01, -3.7405e-02, -4.9212e-02, -2.1102e-02,\n",
      "        -2.9556e-02, -3.4171e-02, -4.0231e-02, -1.7017e-02, -6.7347e-02,\n",
      "        -5.1121e-02, -4.9849e-02, -4.2858e-02, -5.7206e-02, -3.5498e-02,\n",
      "        -2.2533e-02, -5.2661e-02, -5.3515e-02, -2.4666e-02, -1.1493e-01,\n",
      "        -2.3261e-02,  6.1103e-03, -3.5083e-02, -3.4306e-02, -7.2987e-02,\n",
      "        -2.3257e-02, -2.8496e-02, -5.9007e-02, -2.7630e-02, -7.2020e-02,\n",
      "        -3.9159e-02, -4.4350e-02, -1.1111e-02, -1.0414e-02, -6.7582e-03,\n",
      "        -4.4837e-02, -3.2368e-02, -3.3007e-02, -4.8748e-03, -1.3233e-02,\n",
      "        -4.2869e-02,  3.1440e-03, -2.6756e-02,  1.0548e-01, -6.4406e-02,\n",
      "        -1.9947e-02, -2.2722e-02, -2.6712e-03, -6.0724e-02, -5.4049e-02,\n",
      "        -2.2834e-02, -2.3829e-02, -4.0510e-02, -8.9300e-03, -2.1541e-03,\n",
      "        -4.8774e-02, -5.8587e-02, -6.7767e-02, -3.1304e-02, -4.8705e-02,\n",
      "        -5.0249e-02,  9.9318e-02, -4.8286e-02, -2.0225e-02, -2.7785e-02,\n",
      "        -3.4467e-02, -3.2026e-02, -1.3300e-02, -1.9223e-02, -5.3287e-02,\n",
      "        -3.5907e-02, -7.7329e-02, -4.9102e-02, -2.6155e-02, -2.3513e-02,\n",
      "        -7.7463e-03, -3.2800e-02, -2.8559e-02, -4.3823e-02, -1.9141e-02,\n",
      "        -3.8022e-02, -1.2168e-02,  1.4623e-01, -1.0332e-02, -2.3359e-02,\n",
      "        -1.8797e-02, -1.0095e-03, -2.5831e-02, -4.1451e-03, -3.9868e-02,\n",
      "        -4.0413e-02, -4.7642e-02, -2.5505e-02, -3.8872e-02, -1.6609e-02,\n",
      "        -3.0233e-02,  6.7207e-02, -1.6054e-02, -5.1898e-02, -5.5838e-02,\n",
      "        -7.6919e-03, -3.9256e-02, -5.0988e-02, -3.6592e-02, -6.5062e-02,\n",
      "        -5.7295e-02, -3.8011e-02, -2.9790e-02, -2.4095e-02, -5.2060e-02,\n",
      "        -3.0491e-02, -7.4220e-02, -4.1126e-02, -4.3990e-02, -3.2825e-02,\n",
      "        -2.3242e-02, -3.5543e-02,  1.4917e-01, -2.7717e-02, -5.0492e-02,\n",
      "        -5.1189e-02, -2.1554e-02, -1.7559e-02, -1.9493e-02, -1.8185e-02,\n",
      "        -4.1059e-02, -1.2588e-02, -4.0400e-02, -4.7129e-02,  1.9588e-01,\n",
      "        -4.1017e-02, -5.7001e-02, -3.8442e-02, -3.6075e-02, -1.0542e-02,\n",
      "        -2.7664e-02, -3.8738e-02, -7.2411e-02, -4.4538e-02, -1.4289e-02,\n",
      "        -2.3872e-02,  1.3520e-01, -4.7536e-02, -5.2328e-02, -2.6600e-02,\n",
      "         1.5632e-01, -5.6505e-03, -3.5092e-02, -4.4993e-02, -2.8010e-02,\n",
      "        -4.0171e-02, -1.7082e-02, -4.0643e-02, -1.3369e-02, -8.2807e-03,\n",
      "        -5.2438e-02, -1.7884e-02, -2.4462e-02, -3.9255e-02, -3.2312e-02,\n",
      "        -5.5971e-02, -3.0554e-02, -8.8829e-03, -3.6716e-02, -3.9369e-02,\n",
      "        -1.8743e-02, -6.3098e-02, -3.9989e-02, -8.5519e-02, -2.6309e-02,\n",
      "        -1.9008e-02, -3.5191e-02, -2.9083e-02, -3.5827e-02,  4.1654e-03,\n",
      "        -3.3415e-02, -4.4486e-02,  1.9084e-01, -2.5405e-02, -4.6336e-02,\n",
      "        -2.2549e-02, -6.5798e-02, -4.7185e-02, -1.0761e-02, -1.2169e-02,\n",
      "        -1.2368e-03, -5.1535e-02, -3.2792e-02, -3.9293e-02, -8.9737e-02,\n",
      "        -4.7278e-02, -6.6515e-02, -4.0206e-02, -4.8262e-02, -1.3237e-02,\n",
      "        -1.4087e-02, -4.6202e-02, -1.4291e-02, -7.8433e-02, -3.7202e-02,\n",
      "        -3.3343e-02, -2.2485e-02, -8.3118e-02,  1.9715e-03, -1.2830e-02,\n",
      "        -3.8447e-02,  7.7441e-04, -6.3690e-02, -3.1716e-02, -4.4107e-02,\n",
      "        -5.3826e-02, -5.8930e-02, -1.9574e-02, -2.3877e-02,  1.2357e-01,\n",
      "        -3.8992e-02, -3.3977e-02, -2.2063e-02, -3.7203e-02, -3.1987e-02,\n",
      "        -3.0522e-02, -4.2068e-02, -4.4064e-02, -5.9262e-02, -5.6137e-02,\n",
      "        -5.6830e-02, -6.2911e-02, -5.6403e-02, -1.4539e-02, -4.7443e-02,\n",
      "        -6.8500e-02, -2.0878e-02, -1.6078e-02, -5.7247e-02, -1.5334e-02,\n",
      "        -4.2110e-02, -2.4852e-02, -3.8517e-02, -1.6195e-02, -5.6039e-02,\n",
      "        -1.9570e-02, -6.2416e-02, -4.3060e-02,  1.0615e-03, -8.9823e-03,\n",
      "         2.6275e-03, -1.0857e-02, -2.8941e-02, -1.5550e-02,  1.3333e-03,\n",
      "        -5.7949e-02, -3.3295e-02, -1.3646e-02, -4.7077e-02,  2.0965e-01,\n",
      "        -2.0113e-02, -1.3565e-02, -4.0030e-02, -2.8877e-02, -4.1183e-02,\n",
      "        -5.8030e-02, -6.8809e-02, -3.9124e-02, -4.2976e-02, -4.6008e-02,\n",
      "        -4.4379e-02, -1.7982e-02, -6.0801e-02, -2.9766e-02, -2.9201e-02,\n",
      "         1.4046e-01, -5.7121e-02, -5.5465e-02, -1.3983e-02, -1.6590e-02,\n",
      "        -2.8157e-02, -2.5289e-02, -2.7954e-02, -4.2793e-02, -1.4570e-02,\n",
      "         7.7554e-03, -2.6739e-02, -3.1323e-02, -4.2946e-02, -4.5654e-02,\n",
      "        -1.9350e-02, -2.9966e-02, -1.4526e-02, -6.9041e-02, -6.9446e-03,\n",
      "        -5.2297e-02, -8.5494e-03, -4.0864e-02, -1.0699e-02, -5.9404e-02,\n",
      "        -2.8703e-02,  9.8892e-03, -8.2081e-03, -7.4586e-02,  8.0293e-02,\n",
      "        -8.3829e-03, -1.0363e-02, -3.3853e-02, -5.9024e-02, -7.7142e-02,\n",
      "        -4.4235e-02, -1.2252e-02, -1.6914e-02, -4.2241e-02, -5.8330e-02,\n",
      "        -1.4935e-02, -3.8022e-02, -3.5668e-02, -3.1524e-02, -3.4639e-02,\n",
      "        -2.6916e-02, -4.1478e-02, -1.0355e-02, -5.4622e-02, -1.6009e-02,\n",
      "        -2.0172e-02, -3.8968e-02, -5.0805e-02, -3.5805e-02, -6.5145e-02,\n",
      "        -4.1127e-02, -6.0599e-03, -3.8332e-02, -5.8542e-02, -2.1664e-02,\n",
      "        -6.1610e-02, -3.6445e-02, -3.9961e-02, -2.7121e-02, -5.3313e-02,\n",
      "        -1.2429e-02, -1.1236e-02, -3.2143e-02, -6.1141e-02, -1.7770e-02,\n",
      "        -4.4894e-02, -4.1416e-02, -1.2879e-02, -4.0679e-02, -2.2379e-02,\n",
      "        -4.1089e-02, -8.5063e-03, -5.4047e-02, -1.6098e-02, -3.2982e-02,\n",
      "        -2.6400e-02, -5.6385e-02, -5.5933e-03, -5.0784e-02, -5.7643e-02,\n",
      "         7.2013e-05, -7.0556e-02, -6.1635e-02, -5.6684e-02, -1.8122e-02,\n",
      "        -3.0334e-02, -2.0437e-02, -5.1609e-02, -3.9879e-02, -2.2843e-02,\n",
      "        -1.9990e-02, -5.1400e-02, -2.6357e-02])\n"
     ]
    }
   ],
   "source": [
    "print(list(weights.items())[0][1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
