{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Later\n",
    "* BERT Q&A code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simply using the functions from tutorial for now\n",
    "class BertQandAAnalyzer():\n",
    "  \n",
    "#   device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "  def __init__(self, model_path):\n",
    "    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    self.model = BertForQuestionAnswering.from_pretrained(model_path)\n",
    "    self.model.to(self.device)\n",
    "    self.model.eval()\n",
    "    self.model.zero_grad()\n",
    "    self.tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "    self.ref_token_id = self.tokenizer.pad_token_id # A token used for generating token reference\n",
    "    self.sep_token_id = self.tokenizer.sep_token_id # A token used as a separator between question and text and it is also added to the end of the text.\n",
    "    self.cls_token_id = self.tokenizer.cls_token_id # A token used for prepending to the concatenated question-text word sequence\n",
    "\n",
    "  \n",
    "  # Preprocessing\n",
    "\n",
    "  def _predict(self, inputs, token_type_ids=None, position_ids=None, attention_mask=None):\n",
    "      output = self.model(inputs, token_type_ids=token_type_ids,\n",
    "                  position_ids=position_ids, attention_mask=attention_mask, )\n",
    "      return output.start_logits, output.end_logits\n",
    "  \n",
    "  def _squad_pos_forward_func(self, inputs, token_type_ids=None, position_ids=None, attention_mask=None, position=0):\n",
    "    pred = self._predict(inputs,\n",
    "                   token_type_ids=token_type_ids,\n",
    "                   position_ids=position_ids,\n",
    "                   attention_mask=attention_mask)\n",
    "    pred = pred[position]\n",
    "    return pred.max(1).values\n",
    "  \n",
    "  \n",
    "  def _construct_input_ref_pair(self, question, text, ref_token_id, sep_token_id, cls_token_id):\n",
    "    question_ids = self.tokenizer.encode(question, add_special_tokens=False)\n",
    "    text_ids = self.tokenizer.encode(text, add_special_tokens=False)\n",
    "\n",
    "    # construct input token ids\n",
    "    input_ids = [cls_token_id] + question_ids + [sep_token_id] + text_ids + [sep_token_id]\n",
    "\n",
    "    # construct reference token ids \n",
    "    ref_input_ids = [cls_token_id] + [ref_token_id] * len(question_ids) + [sep_token_id] + \\\n",
    "        [ref_token_id] * len(text_ids) + [sep_token_id]\n",
    "\n",
    "    return torch.tensor([input_ids], device=self.device), torch.tensor([ref_input_ids], device=self.device), len(question_ids)\n",
    "\n",
    "  def _construct_input_ref_token_type_pair(self, input_ids, sep_ind=0):\n",
    "      seq_len = input_ids.size(1)\n",
    "      token_type_ids = torch.tensor([[0 if i <= sep_ind else 1 for i in range(seq_len)]], device=self.device)\n",
    "      ref_token_type_ids = torch.zeros_like(token_type_ids, device=self.device)# * -1\n",
    "      return token_type_ids, ref_token_type_ids\n",
    "\n",
    "  def _construct_input_ref_pos_id_pair(self, input_ids):\n",
    "      seq_length = input_ids.size(1)\n",
    "      position_ids = torch.arange(seq_length, dtype=torch.long, device=self.device)\n",
    "      # we could potentially also use random permutation with `torch.randperm(seq_length, device=device)`\n",
    "      ref_position_ids = torch.zeros(seq_length, dtype=torch.long, device=self.device)\n",
    "\n",
    "      position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "      ref_position_ids = ref_position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "      return position_ids, ref_position_ids\n",
    "    \n",
    "  def _construct_attention_mask(self, input_ids):\n",
    "      return torch.ones_like(input_ids)\n",
    "\n",
    "  def _construct_whole_bert_embeddings(self, input_ids, ref_input_ids, \\\n",
    "                                      token_type_ids=None, ref_token_type_ids=None, \\\n",
    "                                      position_ids=None, ref_position_ids=None):\n",
    "      input_embeddings = self.model.bert.embeddings(input_ids, token_type_ids=token_type_ids, position_ids=position_ids)\n",
    "      ref_input_embeddings = self.model.bert.embeddings(ref_input_ids, token_type_ids=ref_token_type_ids, position_ids=ref_position_ids)\n",
    "      \n",
    "      return input_embeddings, ref_input_embeddings\n",
    "\n",
    "  def _summarize_attributions(self, attributions):\n",
    "      attributions = attributions.sum(dim=-1).squeeze(0)\n",
    "      attributions = attributions / torch.norm(attributions)\n",
    "      return attributions\n",
    "\n",
    "  def __call__(self, question, text, ground_truth, visualize=True):\n",
    "\n",
    "    input_ids, ref_input_ids, sep_id = self._construct_input_ref_pair(question, text, self.ref_token_id, self.sep_token_id, self.cls_token_id)\n",
    "    token_type_ids, ref_token_type_ids = self._construct_input_ref_token_type_pair(input_ids, sep_id)\n",
    "    position_ids, ref_position_ids = self._construct_input_ref_pos_id_pair(input_ids)\n",
    "    attention_mask = self._construct_attention_mask(input_ids)\n",
    "\n",
    "    indices = input_ids[0].detach().tolist()\n",
    "    all_tokens = self.tokenizer.convert_ids_to_tokens(indices)\n",
    "\n",
    "    # ground_truth = 'to include, empower and support humans of all kinds'\n",
    "\n",
    "    ground_truth_tokens = self.tokenizer.encode(ground_truth, add_special_tokens=False)\n",
    "    ground_truth_end_ind = indices.index(ground_truth_tokens[-1])\n",
    "    ground_truth_start_ind = ground_truth_end_ind - len(ground_truth_tokens) + 1\n",
    "    \n",
    "    start_scores, end_scores = self._predict(input_ids, \\\n",
    "                                    token_type_ids=token_type_ids, \\\n",
    "                                    position_ids=position_ids, \\\n",
    "                                    attention_mask=attention_mask)\n",
    "\n",
    "    print('Question: ', question)\n",
    "    print('Predicted Answer: ', ' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1]))\n",
    "\n",
    "    lig = LayerIntegratedGradients(self._squad_pos_forward_func, self.model.bert.embeddings)\n",
    "\n",
    "    attributions_start, delta_start = lig.attribute(inputs=input_ids,\n",
    "                                      baselines=ref_input_ids,\n",
    "                                      additional_forward_args=(token_type_ids, position_ids, attention_mask, 0),\n",
    "                                      return_convergence_delta=True)\n",
    "    attributions_end, delta_end = lig.attribute(inputs=input_ids, baselines=ref_input_ids,\n",
    "                                    additional_forward_args=(token_type_ids, position_ids, attention_mask, 1),\n",
    "                                    return_convergence_delta=True)\n",
    "\n",
    "    attributions_start_sum = self._summarize_attributions(attributions_start)\n",
    "    attributions_end_sum = self._summarize_attributions(attributions_end)\n",
    "\n",
    "    if visualize:\n",
    "      # storing couple samples in an array for visualization purposes\n",
    "      start_position_vis = viz.VisualizationDataRecord(\n",
    "                              attributions_start_sum,\n",
    "                              torch.max(torch.softmax(start_scores[0], dim=0)),\n",
    "                              torch.argmax(start_scores),\n",
    "                              torch.argmax(start_scores),\n",
    "                              str(ground_truth_start_ind),\n",
    "                              attributions_start_sum.sum(),       \n",
    "                              all_tokens,\n",
    "                              delta_start)\n",
    "\n",
    "      end_position_vis = viz.VisualizationDataRecord(\n",
    "                              attributions_end_sum,\n",
    "                              torch.max(torch.softmax(end_scores[0], dim=0)),\n",
    "                              torch.argmax(end_scores),\n",
    "                              torch.argmax(end_scores),\n",
    "                              str(ground_truth_end_ind),\n",
    "                              attributions_end_sum.sum(),       \n",
    "                              all_tokens,\n",
    "                              delta_end)\n",
    "\n",
    "      print('\\033[1m', 'Visualizations For Start Position', '\\033[0m')\n",
    "      viz.visualize_text([start_position_vis])\n",
    "\n",
    "      print('\\033[1m', 'Visualizations For End Position', '\\033[0m')\n",
    "      viz.visualize_text([end_position_vis])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
