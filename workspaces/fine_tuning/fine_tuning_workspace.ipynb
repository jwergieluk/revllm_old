{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (AdamW,\n",
    "                        AutoTokenizer,\n",
    "                        AutoModelForSequenceClassification,\n",
    "                        Trainer\n",
    "                        )\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([8, 67]),\n",
       " 'token_type_ids': torch.Size([8, 67]),\n",
       " 'attention_mask': torch.Size([8, 67]),\n",
       " 'labels': torch.Size([8])}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FOR LATER: raw_datasets = load_dataset(\"squad\") for later\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "raw_train_dataset = raw_datasets[\"train\"]\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "tokenized_sentences_1 = tokenizer(raw_train_dataset['sentence1'])\n",
    "tokenized_sentences_2 = tokenizer(raw_train_dataset['sentence2'])\n",
    "\n",
    "tokenized_dataset = tokenizer(\n",
    "    raw_train_dataset[\"sentence1\"],\n",
    "    raw_train_dataset[\"sentence2\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    ")\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "samples = tokenized_datasets[\"train\"][:8]\n",
    "samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\n",
    "[len(x) for x in samples[\"input_ids\"]]\n",
    "\n",
    "batch = data_collator(samples)\n",
    "{k: v.shape for k, v in batch.items()}\n",
    "\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "# batch = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# batch['labels'] = torch.tensor([1, 1])\n",
    "\n",
    "# optimizer = AdamW(model.parameters())\n",
    "\n",
    "# loss = model(**batch).loss\n",
    "# loss.backward()\n",
    "# optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\"test-trainer\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 16:08:04] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 16:08:04] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 16:08:04] Tracking Nvidia GPU via pynvml\n"
     ]
    },
    {
     "ename": "NVMLError_NotSupported",
     "evalue": "Not Supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNVMLError_NotSupported\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m/home/george/Desktop/llm_bootcamp/revllm_repo/workspaces/explainability_packages_workspace/fine_tuning_workspace.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/george/Desktop/llm_bootcamp/revllm_repo/workspaces/explainability_packages_workspace/fine_tuning_workspace.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/george/Desktop/llm_bootcamp/revllm_repo/workspaces/explainability_packages_workspace/fine_tuning_workspace.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     model,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/george/Desktop/llm_bootcamp/revllm_repo/workspaces/explainability_packages_workspace/fine_tuning_workspace.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     training_args,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/george/Desktop/llm_bootcamp/revllm_repo/workspaces/explainability_packages_workspace/fine_tuning_workspace.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     train_dataset\u001b[39m=\u001b[39;49mtokenized_datasets[\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/george/Desktop/llm_bootcamp/revllm_repo/workspaces/explainability_packages_workspace/fine_tuning_workspace.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     eval_dataset\u001b[39m=\u001b[39;49mtokenized_datasets[\u001b[39m\"\u001b[39;49m\u001b[39mvalidation\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/george/Desktop/llm_bootcamp/revllm_repo/workspaces/explainability_packages_workspace/fine_tuning_workspace.ipynb#X16sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     data_collator\u001b[39m=\u001b[39;49mdata_collator,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/george/Desktop/llm_bootcamp/revllm_repo/workspaces/explainability_packages_workspace/fine_tuning_workspace.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     tokenizer\u001b[39m=\u001b[39;49mtokenizer,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/george/Desktop/llm_bootcamp/revllm_repo/workspaces/explainability_packages_workspace/fine_tuning_workspace.ipynb#X16sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/captum/lib/python3.11/site-packages/transformers/trainer.py:687\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    685\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel_names \u001b[39m=\u001b[39m default_label_names \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mlabel_names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mlabel_names\n\u001b[1;32m    686\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcan_return_loss \u001b[39m=\u001b[39m can_return_loss(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m--> 687\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcallback_handler\u001b[39m.\u001b[39;49mon_init_end(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstate, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcontrol)\n\u001b[1;32m    689\u001b[0m \u001b[39m# Internal variables to help with automatic batch size reduction\u001b[39;00m\n\u001b[1;32m    690\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size \u001b[39m=\u001b[39m args\u001b[39m.\u001b[39mtrain_batch_size\n",
      "File \u001b[0;32m~/anaconda3/envs/captum/lib/python3.11/site-packages/transformers/trainer_callback.py:358\u001b[0m, in \u001b[0;36mCallbackHandler.on_init_end\u001b[0;34m(self, args, state, control)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_init_end\u001b[39m(\u001b[39mself\u001b[39m, args: TrainingArguments, state: TrainerState, control: TrainerControl):\n\u001b[0;32m--> 358\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall_event(\u001b[39m\"\u001b[39;49m\u001b[39mon_init_end\u001b[39;49m\u001b[39m\"\u001b[39;49m, args, state, control)\n",
      "File \u001b[0;32m~/anaconda3/envs/captum/lib/python3.11/site-packages/transformers/trainer_callback.py:406\u001b[0m, in \u001b[0;36mCallbackHandler.call_event\u001b[0;34m(self, event, args, state, control, **kwargs)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall_event\u001b[39m(\u001b[39mself\u001b[39m, event, args, state, control, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    405\u001b[0m     \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[0;32m--> 406\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(callback, event)(\n\u001b[1;32m    407\u001b[0m             args,\n\u001b[1;32m    408\u001b[0m             state,\n\u001b[1;32m    409\u001b[0m             control,\n\u001b[1;32m    410\u001b[0m             model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel,\n\u001b[1;32m    411\u001b[0m             tokenizer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer,\n\u001b[1;32m    412\u001b[0m             optimizer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer,\n\u001b[1;32m    413\u001b[0m             lr_scheduler\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlr_scheduler,\n\u001b[1;32m    414\u001b[0m             train_dataloader\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_dataloader,\n\u001b[1;32m    415\u001b[0m             eval_dataloader\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meval_dataloader,\n\u001b[1;32m    416\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    417\u001b[0m         )\n\u001b[1;32m    418\u001b[0m         \u001b[39m# A Callback can skip the return of `control` if it doesn't change it.\u001b[39;00m\n\u001b[1;32m    419\u001b[0m         \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/captum/lib/python3.11/site-packages/transformers/integrations/integration_utils.py:1416\u001b[0m, in \u001b[0;36mCodeCarbonCallback.on_init_end\u001b[0;34m(self, args, state, control, **kwargs)\u001b[0m\n\u001b[1;32m   1413\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_init_end\u001b[39m(\u001b[39mself\u001b[39m, args, state, control, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1414\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtracker \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m state\u001b[39m.\u001b[39mis_local_process_zero:\n\u001b[1;32m   1415\u001b[0m         \u001b[39m# CodeCarbon will automatically handle environment variables for configuration\u001b[39;00m\n\u001b[0;32m-> 1416\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtracker \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_codecarbon\u001b[39m.\u001b[39;49mEmissionsTracker(output_dir\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49moutput_dir)\n",
      "File \u001b[0;32m~/anaconda3/envs/captum/lib/python3.11/site-packages/codecarbon/emissions_tracker.py:284\u001b[0m, in \u001b[0;36mBaseEmissionsTracker.__init__\u001b[0;34m(self, project_name, measure_power_secs, api_call_interval, api_endpoint, api_key, output_dir, output_file, save_to_file, save_to_api, save_to_logger, logging_logger, save_to_prometheus, prometheus_url, gpu_ids, emissions_endpoint, experiment_id, experiment_name, co2_signal_api_token, tracking_mode, log_level, on_csv_write, logger_preamble, default_cpu_power, pue)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[39mif\u001b[39;00m gpu\u001b[39m.\u001b[39mis_gpu_details_available():\n\u001b[1;32m    283\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mTracking Nvidia GPU via pynvml\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 284\u001b[0m     gpu_devices \u001b[39m=\u001b[39m GPU\u001b[39m.\u001b[39;49mfrom_utils(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_gpu_ids)\n\u001b[1;32m    285\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hardware\u001b[39m.\u001b[39mappend(gpu_devices)\n\u001b[1;32m    286\u001b[0m     gpu_names \u001b[39m=\u001b[39m [n[\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m gpu_devices\u001b[39m.\u001b[39mdevices\u001b[39m.\u001b[39mget_gpu_static_info()]\n",
      "File \u001b[0;32m~/anaconda3/envs/captum/lib/python3.11/site-packages/codecarbon/external/hardware.py:121\u001b[0m, in \u001b[0;36mGPU.from_utils\u001b[0;34m(cls, gpu_ids)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    120\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_utils\u001b[39m(\u001b[39mcls\u001b[39m, gpu_ids: Optional[List] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mGPU\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 121\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m(gpu_ids\u001b[39m=\u001b[39;49mgpu_ids)\n",
      "File \u001b[0;32m<string>:4\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, gpu_ids)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/captum/lib/python3.11/site-packages/codecarbon/external/hardware.py:63\u001b[0m, in \u001b[0;36mGPU.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__post_init__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m---> 63\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevices \u001b[39m=\u001b[39m AllGPUDevices()\n\u001b[1;32m     64\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_gpus \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevices\u001b[39m.\u001b[39mdevice_count\n\u001b[1;32m     65\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_total_power \u001b[39m=\u001b[39m Power(\n\u001b[1;32m     66\u001b[0m         \u001b[39m0\u001b[39m  \u001b[39m# It will be 0 until we call for the first time measure_power_and_energy\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/captum/lib/python3.11/site-packages/codecarbon/core/gpu.py:208\u001b[0m, in \u001b[0;36mAllGPUDevices.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_count):\n\u001b[1;32m    207\u001b[0m     handle \u001b[39m=\u001b[39m pynvml\u001b[39m.\u001b[39mnvmlDeviceGetHandleByIndex(i)\n\u001b[0;32m--> 208\u001b[0m     gpu_device \u001b[39m=\u001b[39m GPUDevice(handle\u001b[39m=\u001b[39;49mhandle, gpu_index\u001b[39m=\u001b[39;49mi)\n\u001b[1;32m    209\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevices\u001b[39m.\u001b[39mappend(gpu_device)\n",
      "File \u001b[0;32m<string>:8\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, handle, gpu_index, energy_delta, power, last_energy)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/captum/lib/python3.11/site-packages/codecarbon/core/gpu.py:46\u001b[0m, in \u001b[0;36mGPUDevice.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__post_init__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m---> 46\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_energy \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_energy_kwh()\n\u001b[1;32m     47\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_static_details()\n",
      "File \u001b[0;32m~/anaconda3/envs/captum/lib/python3.11/site-packages/codecarbon/core/gpu.py:50\u001b[0m, in \u001b[0;36mGPUDevice._get_energy_kwh\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_energy_kwh\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m---> 50\u001b[0m     \u001b[39mreturn\u001b[39;00m Energy\u001b[39m.\u001b[39mfrom_millijoules(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_total_energy_consumption())\n",
      "File \u001b[0;32m~/anaconda3/envs/captum/lib/python3.11/site-packages/codecarbon/core/gpu.py:117\u001b[0m, in \u001b[0;36mGPUDevice._get_total_energy_consumption\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_total_energy_consumption\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    114\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Returns total energy consumption for this GPU in millijoules (mJ) since the driver was last reloaded\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[39m    https://docs.nvidia.com/deploy/nvml-api/group__nvmlDeviceQueries.html#group__nvmlDeviceQueries_1g732ab899b5bd18ac4bfb93c02de4900a\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[39mreturn\u001b[39;00m pynvml\u001b[39m.\u001b[39;49mnvmlDeviceGetTotalEnergyConsumption(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle)\n",
      "File \u001b[0;32m~/anaconda3/envs/captum/lib/python3.11/site-packages/pynvml/nvml.py:2411\u001b[0m, in \u001b[0;36mnvmlDeviceGetTotalEnergyConsumption\u001b[0;34m(handle)\u001b[0m\n\u001b[1;32m   2409\u001b[0m fn \u001b[39m=\u001b[39m _nvmlGetFunctionPointer(\u001b[39m\"\u001b[39m\u001b[39mnvmlDeviceGetTotalEnergyConsumption\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   2410\u001b[0m ret \u001b[39m=\u001b[39m fn(handle, byref(c_millijoules))\n\u001b[0;32m-> 2411\u001b[0m _nvmlCheckReturn(ret)\n\u001b[1;32m   2412\u001b[0m \u001b[39mreturn\u001b[39;00m c_millijoules\u001b[39m.\u001b[39mvalue\n",
      "File \u001b[0;32m~/anaconda3/envs/captum/lib/python3.11/site-packages/pynvml/nvml.py:833\u001b[0m, in \u001b[0;36m_nvmlCheckReturn\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_nvmlCheckReturn\u001b[39m(ret):\n\u001b[1;32m    832\u001b[0m     \u001b[39mif\u001b[39;00m (ret \u001b[39m!=\u001b[39m NVML_SUCCESS):\n\u001b[0;32m--> 833\u001b[0m         \u001b[39mraise\u001b[39;00m NVMLError(ret)\n\u001b[1;32m    834\u001b[0m     \u001b[39mreturn\u001b[39;00m ret\n",
      "\u001b[0;31mNVMLError_NotSupported\u001b[0m: Not Supported"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "captum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
