{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare with Actual Distilbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from distilbert_by_hand import DistilBertByHand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# their_transformer.eval()  # evaluation mode\n",
    "distilbert_weights = their_transformer.state_dict()\n",
    "# for key in distilbert_weights.keys():\n",
    "#     if 'layer.0' in key:\n",
    "#         print(key, distilbert_weights[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The cat sat on the mat.\"\n",
    "\n",
    "# My implementation\n",
    "my_transformer = DistilBertByHand()\n",
    "\n",
    "# Actual implementation\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "their_transformer = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "# Compare the outputs\n",
    "my_output = my_transformer(sentence)\n",
    "with torch.no_grad():  \n",
    "    their_output = their_transformer(**inputs)\n",
    "\n",
    "print(my_output)\n",
    "print(their_output[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "their_transformer.eval()  # evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head_divided_weight_matrix shape:  torch.Size([12, 64, 768])\n",
      "head_divided_weight_matrix shape:  torch.Size([12, 64, 768])\n",
      "head_divided_weight_matrix shape:  torch.Size([12, 64, 768])\n",
      "Q shape:  torch.Size([12, 9, 64])\n",
      "K shape:  torch.Size([12, 9, 64])\n",
      "V shape:  torch.Size([12, 9, 64])\n",
      "A shape:  torch.Size([12, 9, 9])\n",
      "V shape:  torch.Size([12, 9, 64])\n",
      "V shape:  torch.Size([9, 768])\n",
      "tensor([[ 0.4260,  0.8195,  1.9080,  ...,  0.2898, -0.9066, -0.3869],\n",
      "        [ 0.2259, -0.0940,  0.1709,  ..., -0.9786, -0.3274, -0.0714],\n",
      "        [ 0.1300, -0.2636,  0.1164,  ...,  0.2267,  0.3086,  0.2580],\n",
      "        ...,\n",
      "        [-0.6407,  0.3197,  0.5364,  ..., -0.5793,  0.0201,  0.2133],\n",
      "        [ 0.1996, -0.1013,  1.0933,  ..., -0.0736,  0.0613, -0.3893],\n",
      "        [ 0.6916,  0.4632,  1.5045,  ...,  0.1637, -0.3426,  0.0680]])\n",
      "tensor([[-0.2713, -0.0781, -0.0216,  ..., -0.0853,  0.4197,  0.1664],\n",
      "        [-0.2253,  0.0514, -0.1776,  ..., -0.0192,  1.0410, -0.4474],\n",
      "        [-0.1074, -0.0692,  0.1663,  ..., -0.2652,  0.3477,  0.3385],\n",
      "        ...,\n",
      "        [ 0.2359, -0.0804, -0.0106,  ..., -0.1970,  0.3074,  0.1018],\n",
      "        [ 0.3169, -0.1647, -0.3697,  ...,  0.0730,  0.0621, -0.6574],\n",
      "        [ 0.3583,  0.1961,  0.0775,  ..., -0.1091,  0.0866, -0.5101]])\n"
     ]
    }
   ],
   "source": [
    "# Compare the outputs\n",
    "\n",
    "\n",
    "# My ouputs\n",
    "my_output = my_transformer(sentence)\n",
    "my_hidden_states = my_transformer.hidden_states\n",
    "my_attention_weights = my_transformer.attention_weights\n",
    "print(my_output)\n",
    "\n",
    "# Their outputs\n",
    "with torch.no_grad():  \n",
    "    their_output = their_transformer(**inputs)\n",
    "    their_attention_weights = their_output.attentions\n",
    "print(their_output[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 9, 9])\n",
      "torch.Size([12, 9, 9])\n",
      "RMSE:  tensor(0.0002)\n"
     ]
    }
   ],
   "source": [
    "print(my_attention_weights[0].shape)\n",
    "print(their_attention_weights[0][0].shape)\n",
    "\n",
    "their_A = their_attention_weights[0][0]\n",
    "my_A = my_attention_weights[0]\n",
    "rmse(their_A, my_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigate where I diverge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------\n",
      "Embedding Layer\n",
      "RMSE:  tensor(0.0004)\n",
      "-------------------------------------------------------------\n",
      "Layer  0\n",
      "Attention\n",
      "RMSE:  tensor(0.0002)\n",
      "Hidden Layer Output\n",
      "RMSE:  tensor(0.5929)\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Investigate where we diverge\n",
    "\n",
    "# For space, I stop at the first layer because I can already see the difference, which lies somewhere between\n",
    "# the attention weights and the output of the first layer\n",
    "print('-------------------------------------------------------------')\n",
    "for i in range(2): #7):\n",
    "    if i == 0:\n",
    "        print('Embedding Layer')\n",
    "        rmse(my_hidden_states[i],their_output['hidden_states'][i][0])\n",
    "        print('-------------------------------------------------------------')\n",
    "    else:\n",
    "        print('Layer ', i-1)\n",
    "        print('Attention')\n",
    "        rmse(my_attention_weights[i-1],their_attention_weights[i-1][0])\n",
    "        print('Hidden Layer Output')\n",
    "        rmse(my_hidden_states[i],their_output['hidden_states'][i][0])\n",
    "        print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So it's something in between attention and the end of the block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hook Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I've narrowed it down to the residual (output of self-attention before adding to X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  tensor(0.0002)\n"
     ]
    }
   ],
   "source": [
    "their_A = their_attention_weights[0][0]\n",
    "my_A = my_attention_weights[0]\n",
    "rmse(their_A, my_A)\n",
    "# their_V  = \n",
    "\n",
    "# my_V = my_transformer.mine_for_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigate AV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def hook(module, input, output):\n",
    "#     global hook_output\n",
    "#     hook_output = output\n",
    "\n",
    "\n",
    "# model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "# hook_handle = model.transformer.layer[0].attention.v_lin.register_forward_hook(hook)\n",
    "\n",
    "# tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "# inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     output = model(**inputs)\n",
    "    \n",
    "# my_V_weights = my_transformer.mine_for_comparison\n",
    "# my_V_weights = my_V_weights.view(1,9,768)\n",
    "# their_V_weights = hook_output\n",
    "\n",
    "# hook_handle.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(my_V_weights.shape)\n",
    "# print(their_V_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'transformer.layer.0.attention.q_lin.weight', 'transformer.layer.0.attention.q_lin.bias', 'transformer.layer.0.attention.k_lin.weight', 'transformer.layer.0.attention.k_lin.bias', 'transformer.layer.0.attention.v_lin.weight', 'transformer.layer.0.attention.v_lin.bias', 'transformer.layer.0.attention.out_lin.weight', 'transformer.layer.0.attention.out_lin.bias', 'transformer.layer.0.sa_layer_norm.weight', 'transformer.layer.0.sa_layer_norm.bias', 'transformer.layer.0.ffn.lin1.weight', 'transformer.layer.0.ffn.lin1.bias', 'transformer.layer.0.ffn.lin2.weight', 'transformer.layer.0.ffn.lin2.bias', 'transformer.layer.0.output_layer_norm.weight', 'transformer.layer.0.output_layer_norm.bias', 'transformer.layer.1.attention.q_lin.weight', 'transformer.layer.1.attention.q_lin.bias', 'transformer.layer.1.attention.k_lin.weight', 'transformer.layer.1.attention.k_lin.bias', 'transformer.layer.1.attention.v_lin.weight', 'transformer.layer.1.attention.v_lin.bias', 'transformer.layer.1.attention.out_lin.weight', 'transformer.layer.1.attention.out_lin.bias', 'transformer.layer.1.sa_layer_norm.weight', 'transformer.layer.1.sa_layer_norm.bias', 'transformer.layer.1.ffn.lin1.weight', 'transformer.layer.1.ffn.lin1.bias', 'transformer.layer.1.ffn.lin2.weight', 'transformer.layer.1.ffn.lin2.bias', 'transformer.layer.1.output_layer_norm.weight', 'transformer.layer.1.output_layer_norm.bias', 'transformer.layer.2.attention.q_lin.weight', 'transformer.layer.2.attention.q_lin.bias', 'transformer.layer.2.attention.k_lin.weight', 'transformer.layer.2.attention.k_lin.bias', 'transformer.layer.2.attention.v_lin.weight', 'transformer.layer.2.attention.v_lin.bias', 'transformer.layer.2.attention.out_lin.weight', 'transformer.layer.2.attention.out_lin.bias', 'transformer.layer.2.sa_layer_norm.weight', 'transformer.layer.2.sa_layer_norm.bias', 'transformer.layer.2.ffn.lin1.weight', 'transformer.layer.2.ffn.lin1.bias', 'transformer.layer.2.ffn.lin2.weight', 'transformer.layer.2.ffn.lin2.bias', 'transformer.layer.2.output_layer_norm.weight', 'transformer.layer.2.output_layer_norm.bias', 'transformer.layer.3.attention.q_lin.weight', 'transformer.layer.3.attention.q_lin.bias', 'transformer.layer.3.attention.k_lin.weight', 'transformer.layer.3.attention.k_lin.bias', 'transformer.layer.3.attention.v_lin.weight', 'transformer.layer.3.attention.v_lin.bias', 'transformer.layer.3.attention.out_lin.weight', 'transformer.layer.3.attention.out_lin.bias', 'transformer.layer.3.sa_layer_norm.weight', 'transformer.layer.3.sa_layer_norm.bias', 'transformer.layer.3.ffn.lin1.weight', 'transformer.layer.3.ffn.lin1.bias', 'transformer.layer.3.ffn.lin2.weight', 'transformer.layer.3.ffn.lin2.bias', 'transformer.layer.3.output_layer_norm.weight', 'transformer.layer.3.output_layer_norm.bias', 'transformer.layer.4.attention.q_lin.weight', 'transformer.layer.4.attention.q_lin.bias', 'transformer.layer.4.attention.k_lin.weight', 'transformer.layer.4.attention.k_lin.bias', 'transformer.layer.4.attention.v_lin.weight', 'transformer.layer.4.attention.v_lin.bias', 'transformer.layer.4.attention.out_lin.weight', 'transformer.layer.4.attention.out_lin.bias', 'transformer.layer.4.sa_layer_norm.weight', 'transformer.layer.4.sa_layer_norm.bias', 'transformer.layer.4.ffn.lin1.weight', 'transformer.layer.4.ffn.lin1.bias', 'transformer.layer.4.ffn.lin2.weight', 'transformer.layer.4.ffn.lin2.bias', 'transformer.layer.4.output_layer_norm.weight', 'transformer.layer.4.output_layer_norm.bias', 'transformer.layer.5.attention.q_lin.weight', 'transformer.layer.5.attention.q_lin.bias', 'transformer.layer.5.attention.k_lin.weight', 'transformer.layer.5.attention.k_lin.bias', 'transformer.layer.5.attention.v_lin.weight', 'transformer.layer.5.attention.v_lin.bias', 'transformer.layer.5.attention.out_lin.weight', 'transformer.layer.5.attention.out_lin.bias', 'transformer.layer.5.sa_layer_norm.weight', 'transformer.layer.5.sa_layer_norm.bias', 'transformer.layer.5.ffn.lin1.weight', 'transformer.layer.5.ffn.lin1.bias', 'transformer.layer.5.ffn.lin2.weight', 'transformer.layer.5.ffn.lin2.bias', 'transformer.layer.5.output_layer_norm.weight', 'transformer.layer.5.output_layer_norm.bias'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distilbert_weights.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768, 768])\n"
     ]
    }
   ],
   "source": [
    "print(distilbert_weights['transformer.layer.0.attention.q_lin.bias'].shape)\n",
    "print(distilbert_weights['transformer.layer.0.attention.k_lin.weight'].shape)\n",
    "print(distilbert_weights['transformer.layer.0.attention.v_lin.weight'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_V_weights = my_V_weights.squeeze(1)\n",
    "# print(my_V_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmse(my_V_weights,their_V_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# their_transformer.transformer.layer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hook_handle = model.transformer.layer[0].attention.v_lin.register_forward_hook(hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W_out_lin = distilbert_weights['transformer.layer.0.attention.out_lin.weight']\n",
    "# b_out_lin = distilbert_weights['transformer.layer.0.attention.out_lin.bias']\n",
    "# b_out_lin_matrix = b_out_lin.repeat(len(inputs), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W_out_lin + b_out_lin"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
